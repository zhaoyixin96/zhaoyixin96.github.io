<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZhaoYixin&#39;s blog</title>
  
  <subtitle>Quick notes</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhaoyixin.xyz/"/>
  <updated>2020-02-21T06:48:00.421Z</updated>
  <id>https://zhaoyixin.xyz/</id>
  
  <author>
    <name>zhaoyixin</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用VBD共享云硬盘搭建RHCS集群</title>
    <link href="https://zhaoyixin.xyz/2020/02/21/%E4%BD%BF%E7%94%A8VBD%E5%85%B1%E4%BA%AB%E4%BA%91%E7%A1%AC%E7%9B%98%E6%90%AD%E5%BB%BARHCS%E9%9B%86%E7%BE%A4/"/>
    <id>https://zhaoyixin.xyz/2020/02/21/%E4%BD%BF%E7%94%A8VBD%E5%85%B1%E4%BA%AB%E4%BA%91%E7%A1%AC%E7%9B%98%E6%90%AD%E5%BB%BARHCS%E9%9B%86%E7%BE%A4/</id>
    <published>2020-02-21T06:35:49.000Z</published>
    <updated>2020-02-21T06:48:00.421Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>以操作系统为CentOS 6.5的云服务器为例，搭建RHCS（Red Hat Cluster Suite）集群系统，使用GFS2分布式文件系统，实现一个共享盘在多个云服务器之间可共享文件。</p><p>将共享云硬盘挂载给多台云服务器后，需要安装共享文件系统或类似的集群管理系统，才能实现在多台云服务器之间共享文件。</p><h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><p>本次搭建一共三个节点，一个管理节点(ecs-share-003)，两个业务节点(ecs-share-001、ecs-share-002)，都是Centos6.5的云服务器。</p><ul><li>一块VBD型的共享云硬盘</li><li>ecs-share-003:10.110.31.166</li><li>ecs-share-001:10.110.31.29</li><li>ecs-share-002:10.110.31.206</li></ul><p>将共享云硬盘挂载到ecs-share-001和ecs-share-002两台云服务器上。</p><h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><ul><li>1核CPU</li><li>2G内存</li><li>Centos6.5</li><li>内网互通</li></ul><h1 id="搭建流程"><a href="#搭建流程" class="headerlink" title="搭建流程"></a>搭建流程</h1><ol><li>配置云服务器网络</li><li>安装RHCS集群</li><li>创建集群</li><li>配置磁盘</li><li>验证磁盘共享功能</li></ol><h1 id="配置云服务器网络"><a href="#配置云服务器网络" class="headerlink" title="配置云服务器网络"></a>配置云服务器网络</h1><p>本操作在三个节点都要执行，现以ecs-share-001为例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# vi &#x2F;etc&#x2F;hosts</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">#在最后添加以下内容，保存后退出</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">10.110.31.29 ecs-share-001</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">10.110.31.206 ecs-share-002</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">10.110.31.166 ecs-share-003</span></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# cat &#x2F;etc&#x2F;hosts</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">10.110.31.29 ecs-share-001</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">10.110.31.206 ecs-share-002</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">10.110.31.166 ecs-share-003</span></pre></td></tr></table></figure><h1 id="安装RHCS集群"><a href="#安装RHCS集群" class="headerlink" title="安装RHCS集群"></a>安装RHCS集群</h1><p>为业务节点安装ricci软件，为管理节点安装luci软件。</p><ul><li>luci。RHCS集群管理工具的主控端，提供了管理RHCS集群的web页面，管理集群主要是通过跟集群中其他节点上的ricci通信来完成的。</li><li>ricci。RHCS集群管理工具的受控端，安装在集群中的其他节点上，luci就是通过每一个节点上的ricci管理后端。</li></ul><h2 id="安装ricci"><a href="#安装ricci" class="headerlink" title="安装ricci"></a>安装ricci</h2><p>本操作在ecs-share-001和ecs-share-002上来完成，现以ecs-share-001为例。</p><p>下载并安装ricci软件包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# yum install ricci openais cman rgmanager lvm2-cluster gfs2-utils</span></pre></td></tr></table></figure><p>关闭防火墙</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# &#x2F;etc&#x2F;init.d&#x2F;iptables stop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# chkconfig iptables off</span></pre></td></tr></table></figure><p>暂停并关闭ACPI（Advanced Configuration and Power Interface）服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# &#x2F;etc&#x2F;init.d&#x2F;acpid stop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Stopping acpi daemon:                                      [  OK  ]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# chkconfig acpid off</span></pre></td></tr></table></figure><p>设置ricci密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# passwd ricci</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Changing password for user ricci.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">New password: </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">BAD PASSWORD: it is too short</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">BAD PASSWORD: is too simple</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">Retype new password: </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">passwd: all authentication tokens updated successfully.</span></pre></td></tr></table></figure><p>启动ricci</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# &#x2F;etc&#x2F;init.d&#x2F;ricci start</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Starting system message bus:                               [  OK  ]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Starting oddjobd:                                          [  OK  ]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">generating SSL certificates...  done</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Generating NSS database...  done</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">Starting ricci:                                            [  OK  ]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# chkconfig ricci on</span></pre></td></tr></table></figure><p>查看云服务器挂载的磁盘</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# fdisk -l</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Disk &#x2F;dev&#x2F;vda: 42.9 GB, 42949672960 bytes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">16 heads, 63 sectors&#x2F;track, 83220 cylinders</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Units &#x3D; cylinders of 1008 * 512 &#x3D; 516096 bytes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">Sector size (logical&#x2F;physical): 512 bytes &#x2F; 512 bytes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">I&#x2F;O size (minimum&#x2F;optimal): 512 bytes &#x2F; 512 bytes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">Disk identifier: 0x000342fb</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">   Device Boot      Start         End      Blocks   Id  System</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#x2F;dev&#x2F;vda1   *           3        1043      524288   83  Linux</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">Partition 1 does not end on cylinder boundary.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">&#x2F;dev&#x2F;vda2            1043        5204     2097152   82  Linux swap &#x2F; Solaris</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">Partition 2 does not end on cylinder boundary.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">&#x2F;dev&#x2F;vda3            5204       83221    39320576   83  Linux</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">Partition 3 does not end on cylinder boundary.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">Disk &#x2F;dev&#x2F;vdb: 67 MB, 67108864 bytes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">16 heads, 63 sectors&#x2F;track, 130 cylinders</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">Units &#x3D; cylinders of 1008 * 512 &#x3D; 516096 bytes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">Sector size (logical&#x2F;physical): 512 bytes &#x2F; 512 bytes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">I&#x2F;O size (minimum&#x2F;optimal): 512 bytes &#x2F; 512 bytes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">Disk identifier: 0x00000000</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">   Device Boot      Start         End      Blocks   Id  System</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">Disk &#x2F;dev&#x2F;vdc: 21.5 GB, 21474836480 bytes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">16 heads, 63 sectors&#x2F;track, 41610 cylinders</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">Units &#x3D; cylinders of 1008 * 512 &#x3D; 516096 bytes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">Sector size (logical&#x2F;physical): 512 bytes &#x2F; 512 bytes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">I&#x2F;O size (minimum&#x2F;optimal): 512 bytes &#x2F; 512 bytes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">Disk identifier: 0x00000000</span></pre></td></tr></table></figure><p>新建磁盘挂载目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# mkdir &#x2F;mnt&#x2F;gfs_vbd</span></pre></td></tr></table></figure><p>为磁盘创建GFS2分布式文件系统</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# mkfs.gfs2 -p lock_dlm -t mycluster:gfs_vbd -j 4 &#x2F;dev&#x2F;vdc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">This will destroy any data on &#x2F;dev&#x2F;vdc.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">It appears to contain: data</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Are you sure you want to proceed? [y&#x2F;n] y</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">Device:                    &#x2F;dev&#x2F;vdc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">Blocksize:                 4096</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">Device Size                20.00 GB (5242880 blocks)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">Filesystem Size:           20.00 GB (5242878 blocks)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">Journals:                  4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">Resource Groups:           80</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">Locking Protocol:          &quot;lock_dlm&quot;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">Lock Table:                &quot;mycluster:gfs_vbd&quot;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">UUID:                      868002c6-ede7-fc8f-d8b6-b2df3d42bae1</span></pre></td></tr></table></figure><h2 id="安装luci"><a href="#安装luci" class="headerlink" title="安装luci"></a>安装luci</h2><p>本操作在管理节点ecs-share-003上完成。</p><p>下载并安装luci集群软件包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-003 ~]# yum install luci</span></pre></td></tr></table></figure><p>关闭防火墙</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-003 ~]# &#x2F;etc&#x2F;init.d&#x2F;iptables stop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-003 ~]# chkconfig iptables off</span></pre></td></tr></table></figure><p>启动luci</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-003 ~]# &#x2F;etc&#x2F;init.d&#x2F;luci start</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Adding following auto-detected host IDs (IP addresses&#x2F;domain names), corresponding to &#96;ecs-share-003&#39; address, to the configuration of self-managed certificate &#96;&#x2F;var&#x2F;lib&#x2F;luci&#x2F;etc&#x2F;cacert.config&#39; (you can change them by editing &#96;&#x2F;var&#x2F;lib&#x2F;luci&#x2F;etc&#x2F;cacert.config&#39;, removing the generated certificate &#96;&#x2F;var&#x2F;lib&#x2F;luci&#x2F;certs&#x2F;host.pem&#39; and restarting luci):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">(none suitable found, you can still do it manually as mentioned above)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Generating a 2048 bit RSA private key</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">writing new private key to &#39;&#x2F;var&#x2F;lib&#x2F;luci&#x2F;certs&#x2F;host.pem&#39;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">Starting saslauthd:                                        [  OK  ]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">Start luci...                                              [  OK  ]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">Point your web browser to https:&#x2F;&#x2F;ecs-share-003:8084 (or equivalent) to access luci</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-003 ~]# chkconfig luci on</span></pre></td></tr></table></figure><p>luci启动成功后，即可以通过管理RHCS集群的web页面来进行集群的相关配置。</p><h1 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h1><p>打开并登录管理web页面</p><ul><li>地址：<a href="https://10.110.31.166:8084" target="_blank" rel="noopener">https://10.110.31.166:8084</a> </li><li>账号密码：登录云服务的root账号和密码</li></ul><p>选择”Manage Cluster”，点击”Create”创建集群</p><ul><li>Cluser Name：自定义集群名称，例如，mycluster。</li><li>Node Name：分别设置业务节点的名称，例如，ecs-share-0001。</li><li>Paasword：安装ricci时设置的ricci密码。</li></ul><p>添加完第一个节点的信息后，单击”Add another Node”添加第二个节点。具体参数如 图1所示。</p><p>图1 创建集群</p><p><img src="https://blog-1300855606.cos.ap-chengdu.myqcloud.com/VBD_RHCS/VBD_RHCS_01_Create_cluster.png" alt="VBD_RHCS_01_Create_cluster"></p><h1 id="配置磁盘"><a href="#配置磁盘" class="headerlink" title="配置磁盘"></a>配置磁盘</h1><p>创建成功后，选择”Reources”，单击”Add”为集群新建磁盘资源。</p><ul><li>Name：自定义磁盘资源名称，例如，GFS_VBD。</li><li>Mount Point：安装ricci中设置的磁盘挂载目录，例如，/mnt/gfs_vbd。</li><li>Device，FS Label，or UUID：此处以填写安装ricci中磁盘设备名称为例，例如，/dev/vdc。</li><li>Filesystem Type：此处选择安装ricci中设置的分布式文件系统，例如，GFS2。</li></ul><p>具体参数如 图2所示。</p><p>图2 创建磁盘资源</p><p><img src="https://blog-1300855606.cos.ap-chengdu.myqcloud.com/VBD_RHCS/VBD_RHCS_02_Add_Resources.png" alt="VBD_RHCS_02_Add_Resources"></p><p>创建成功后，如 图3所示。</p><p>图3 创建磁盘资源成功</p><p><img src="https://blog-1300855606.cos.ap-chengdu.myqcloud.com/VBD_RHCS/VBD_RHCS_03_Add_Resources_success.png" alt="VBD_RHCS_03_Add_Resources_success"></p><p>然后，选择”Failover Domains”，单击”Add”为集群服务新建故障切换域。</p><p>本例添加了名称为gfs_failover的故障切换域，具体参数如 图4所示。</p><p>图4 创建故障切换域</p><p><img src="https://blog-1300855606.cos.ap-chengdu.myqcloud.com/VBD_RHCS/VBD_RHCS_04_Add_Failover_Domains.png" alt="VBD_RHCS_04_Add_Failover_Domains"></p><p>创建成功后再选择”Service Groups”，单击”Add”为集群中的节点创建服务组。</p><ul><li>Service Name：自定义服务组的名称，例如，GFS_SG_VBD_1。</li><li>Failover Domain：选择创建的故障切换域，例如，gfs_failover。</li><li>Add Resource：选择创建的资源，例如，GFS_VBD。</li></ul><p>本示例添加了名称为GFS_SG_VBD_1的服务组，并添加了GFS_VBD磁盘资源，具体参数如 图5和 图6所示。</p><p>图5 创建服务组</p><p><img src="https://blog-1300855606.cos.ap-chengdu.myqcloud.com/VBD_RHCS/VBD_RHCS_05_Create_SG.png" alt="VBD_RHCS_05_Create_SG"></p><p>图6 添加GFS_VBD磁盘资源</p><p><img src="https://blog-1300855606.cos.ap-chengdu.myqcloud.com/VBD_RHCS/VBD_RHCS_06_Add_GFS_VBD.png" alt="VBD_RHCS_06_Add_GFS_VBD"></p><p>单击”submit”后，选择start on ecs-share-001节点。按同样的操作也为ecs-share-002创建服务组。两个服务组都创建成功后，如 图7 所示。</p><p>图7 创建两个服务组成功</p><p><img src="https://blog-1300855606.cos.ap-chengdu.myqcloud.com/VBD_RHCS/VBD_RHCS_07_Create_SG_success.png" alt="VBD_RHCS_07_Create_SG_success"></p><p>服务组创建完成后，分别查看云服务器ecs-share-001和ecs-share-002的集群状况。以ecs-share-001为例。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# clustat</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Cluster Status for mycluster @ Wed Feb 19 04:19:41 2020</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Member Status: Quorate</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"> Member Name                                            ID   Status</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"> ------ ----                                            ---- ------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"> ecs-share-001                                              1 Online, Local, rgmanager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"> ecs-share-002                                              2 Online, rgmanager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"> Service Name                                  Owner (Last)                                  State         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"> ------- ----                                  ----- ------                                  -----         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"> service:GFS_SG_VBD_1                          ecs-share-001                                 started       </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"> service:GFS_SG_VBD_2                          ecs-share-002                                 started</span></pre></td></tr></table></figure><p>查看磁盘分区及挂载信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# df -h</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#x2F;dev&#x2F;vda3        37G  1.3G   34G   4% &#x2F;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">tmpfs           939M   32M  908M   4% &#x2F;dev&#x2F;shm</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">&#x2F;dev&#x2F;vda1       504M   62M  417M  13% &#x2F;boot</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">&#x2F;dev&#x2F;vdc         20G  518M   20G   3% &#x2F;mnt&#x2F;gfs_vbd</span></pre></td></tr></table></figure><p>其中，“/dev/vdc”即为共享盘设备名，分区成功并已挂载至“/mnt/gfs_vbd”目录下。</p><blockquote><p>注意：如果执行完df -h命令发现分区表里没有挂载的共享盘，请执行“mount 磁盘设备名称 挂载目录” 命令重新挂载磁盘，例如，mount /dev/vdc /mnt/gfs_vbd，挂载后便可与其他云服务器同步使用该共享盘。</p></blockquote><h1 id="验证磁盘共享功能"><a href="#验证磁盘共享功能" class="headerlink" title="验证磁盘共享功能"></a>验证磁盘共享功能</h1><h2 id="ecs-share-001写入内容"><a href="#ecs-share-001写入内容" class="headerlink" title="ecs-share-001写入内容"></a>ecs-share-001写入内容</h2><p>使用root用户登录云服务器ecs-share-001，进入“/mnt/gfs_vbd/”目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 ~]# cd &#x2F;mnt&#x2F;gfs_vbd&#x2F;</span></pre></td></tr></table></figure><p>新建“testshare”文件，并写入以下内容。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 gfs_vbd]# vi testshare</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">001 write</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 gfs_vbd]# cat testshare </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">001 write</span></pre></td></tr></table></figure><h2 id="ecs-share-002查看内容，然后修改内容"><a href="#ecs-share-002查看内容，然后修改内容" class="headerlink" title="ecs-share-002查看内容，然后修改内容"></a>ecs-share-002查看内容，然后修改内容</h2><p>使用root用户登录云服务器ecs-share-002。验证在ecs-share-002的磁盘挂载目录“/mnt/gfs_vbd”能否看到“testshare”文件中的内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-002 ~]# cd &#x2F;mnt&#x2F;gfs_vbd&#x2F;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-002 gfs_vbd]# ls</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">testshare</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-002 gfs_vbd]# cat testshare </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">001 write</span></pre></td></tr></table></figure><p>从显示结果中，可以看到内容同步成功。</p><p>然后修改testshare文件，写入以下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-002 gfs_vbd]# vi testshare</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">#在最后添加以下内容，保存后退出</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">002 write</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-002 gfs_vbd]# cat testshare </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">001 write</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">002 write</span></pre></td></tr></table></figure><h2 id="ecs-share-001查看内容改动"><a href="#ecs-share-001查看内容改动" class="headerlink" title="ecs-share-001查看内容改动"></a>ecs-share-001查看内容改动</h2><p>使用root用户登录云服务器ecs-share-001，查看testshare文件中的内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@ecs-share-001 gfs_vbd]# cat testshare </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">001 write</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">002 write</span></pre></td></tr></table></figure><p>可以看到，内容修改同步成功。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;以操作系统为CentOS 6.5的云服务器为例，搭建RHCS（Red Hat Cluster Suite）集群系统，使用GFS2分布式文件系
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>使用kubeadm搭建一个完整的kubernetes集群</title>
    <link href="https://zhaoyixin.xyz/2020/01/20/%E4%BD%BF%E7%94%A8kubeadm%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84kubernetes%E9%9B%86%E7%BE%A4/"/>
    <id>https://zhaoyixin.xyz/2020/01/20/%E4%BD%BF%E7%94%A8kubeadm%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84kubernetes%E9%9B%86%E7%BE%A4/</id>
    <published>2020-01-20T08:07:04.000Z</published>
    <updated>2020-02-21T06:44:45.693Z</updated>
    
    <content type="html"><![CDATA[<h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><p>本次部署一共三个节点，一个master，两个worker，都是 Ubuntu16.04 的虚拟机。</p><ul><li>master ： 172.31.0.57</li><li>worker01 ： 172.31.0.32</li><li>worker02 ： 172.31.0.53</li></ul><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><ul><li>2核CPU</li><li>8G内存</li><li>40G系统盘+100G数据盘（数据盘用作ceph osd）</li><li>Ubuntu16.04</li><li>内网互通</li></ul><h1 id="部署流程"><a href="#部署流程" class="headerlink" title="部署流程"></a>部署流程</h1><ol><li>在所有节点安装 Docker 和 kubeadm</li><li>部署 Kubernetes Master</li><li>部署容器网络插件</li><li>部署 Kubernetes Worker</li><li>通过 Taint 调整 Master 执行 Pod 的策略</li><li>部署 Dashboard 可视化插件（可选）</li><li>部署容器存储插件</li></ol><h1 id="安装-Docker-和-kubeadm"><a href="#安装-Docker-和-kubeadm" class="headerlink" title="安装 Docker 和 kubeadm"></a>安装 Docker 和 kubeadm</h1><blockquote><p>备注：本次部署都是在root用户下进行操作</p><p>这一步的所有操作在所有节点上都要执行</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# apt-get update &amp;&amp; apt-get install -y apt-transport-https</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">root@master:~# curl https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt&#x2F;doc&#x2F;apt-key.gpg | apt-key add -</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">root@master:~# vi &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;kubernetes.list</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"># 添加以下内容，然后保存退出，使用阿里的镜像源</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">deb https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt&#x2F; kubernetes-xenial main</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">root@master:~# apt update</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">root@master:~# apt -y install docker.io kubeadm</span></pre></td></tr></table></figure><p>在安装 kubeadm 的过程中，kubeadm 和 kubelet、kubectl、kubernetes-cni 这几个二进制文件都会被安装好。</p><p>安装 docker 直接使用 docker.io 的安装源，因为发布的最新的 Docker CE（社区版）往往没有经过 Kubernetes 项目的验证，可能会有兼容性问题。</p><p>另外，后续在执行 kubeadm 命令的时候，会进行一系列的检查工作（“preflight”），其中需要禁用虚拟内存（swap）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# swapoff -a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">root@master:~# sed &#39;s&#x2F;.*swap.*&#x2F;#&amp;&#x2F;&#39; &#x2F;etc&#x2F;fstab</span></pre></td></tr></table></figure><h1 id="部署-Kubernetes-的-Master-节点"><a href="#部署-Kubernetes-的-Master-节点" class="headerlink" title="部署 Kubernetes 的 Master 节点"></a>部署 Kubernetes 的 Master 节点</h1><p>首先查看一下我们本次部署的 kubernetes 的版本信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubeadm config print init-defaults</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">.......</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">.......</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">apiServer:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  timeoutForControlPlane: 4m0s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1beta2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">certificatesDir: &#x2F;etc&#x2F;kubernetes&#x2F;pki</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">clusterName: kubernetes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">controllerManager: &#123;&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">dns:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  type: CoreDNS</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">etcd:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">  local:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    dataDir: &#x2F;var&#x2F;lib&#x2F;etcd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">imageRepository: k8s.gcr.io</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">kind: ClusterConfiguration</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">kubernetesVersion: v1.17.0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">networking:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">  dnsDomain: cluster.local</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">  serviceSubnet: 10.96.0.0&#x2F;12</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">scheduler: &#123;&#125;</span></pre></td></tr></table></figure><blockquote><p>备注：通过配置文件来开启一些实验性功能。</p></blockquote><p>有了这些信息后，接下来编写一个给 kubeadm 用的 YAML 文件（kubeadm.yaml）:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# vi kubeadm.yaml</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1beta2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">imageRepository: registry.aliyuncs.com&#x2F;google_containers</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">kind: ClusterConfiguration</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">controllerManager:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    extraArgs:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        horizontal-pod-autoscaler-sync-period: &quot;10s&quot;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        node-monitor-grace-period: &quot;10s&quot;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">apiServer:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    extraArgs:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        runtime-config: &quot;api&#x2F;all&#x3D;true&quot;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">kubernetesVersion: v1.17.0</span></pre></td></tr></table></figure><p>apiVersion、kind、kubernetesVersion 都可以从上面 print 的返回中得到。</p><p>另外把 imageRepository 配置为 <code>registry.aliyuncs.com/google_containers</code> 阿里的源，因为默认的 <code>k8s.gcr.io</code> 因为一些原因访问不了。</p><p><code>horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot;</code> 意味着，将来部署的 kube-controller-manager 能够使用自定义资源进行自动水平扩展。</p><p>接下来，只需要一句指令，就可以完成 Kubernetes 的部署</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubeadm init --config kubeadm.yaml</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">......</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">......</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">Your Kubernetes control-plane has initialized successfully!</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">To start using your cluster, you need to run the following as a regular user:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  mkdir -p $HOME&#x2F;.kube</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">You should now deploy a pod network to the cluster.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">  https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;addons&#x2F;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">kubeadm join 172.31.0.57:6443 --token 1octc9.3uvkgo5vy1sgr7vy \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    --discovery-token-ca-cert-hash sha256:72fc3bf1a6cc0a159a27bf99518c116845b6a8ae05fb8e78cddc4c7156777c10</span></pre></td></tr></table></figure><p>部署完成后，kubeadm 会生成一行指令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">kubeadm join 172.31.0.57:6443 --token 1octc9.3uvkgo5vy1sgr7vy \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    --discovery-token-ca-cert-hash sha256:72fc3bf1a6cc0a159a27bf99518c116845b6a8ae05fb8e78cddc4c7156777c10</span></pre></td></tr></table></figure><p>这个命令，就是用来给这个Master节点添加更多的工作节点使用的。</p><p>另外，kubeadm 还会提示我们一些配置命令，也要执行一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# mkdir -p $HOME&#x2F;.kube</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">root@master:~# cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">root@master:~# chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span></pre></td></tr></table></figure><p>这样配置的原因是：Kubernetes 集群默认需要加密方式访问。所以将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目<br>录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。</p><p>现在，用 <code>kubectl get</code> 命令查看当前唯一一个节点的状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl get nodes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">NAME     STATUS     ROLES    AGE     VERSION</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">master   NotReady   master   2m26s   v1.17.1</span></pre></td></tr></table></figure><p>输出结果中，master 的状态是 NotReady，排查问题，最重要的手段就是用 <code>kubectl describe</code> 来查看这个节点（Node）对象的详细信息、状态和事件（Event）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl describe node master</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">.....</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">Conditions:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  ----             ------  -----------------                 ------------------                ------                       -------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  MemoryPressure   False   Sun, 19 Jan 2020 11:07:23 +0800   Sun, 19 Jan 2020 11:07:23 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  DiskPressure     False   Sun, 19 Jan 2020 11:07:23 +0800   Sun, 19 Jan 2020 11:07:23 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  PIDPressure      False   Sun, 19 Jan 2020 11:07:23 +0800   Sun, 19 Jan 2020 11:07:23 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  Ready            False   Sun, 19 Jan 2020 11:07:23 +0800   Sun, 19 Jan 2020 11:07:23 +0800   KubeletNotReady              runtime network not ready: NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">.....</span></pre></td></tr></table></figure><p>从输出中，可以看到原因在于为部署任何网络插件，<code>network plugin is not ready: cni config uninitialized</code>。</p><p>我们还可以通过 kubectl 查看这个节点上各个系统 Pod 的状态，其中 kube-system 是 Kubernetes 项目预留给系统 Pod 的工作空间。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl get pods -n kube-system </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">NAME                             READY   STATUS    RESTARTS   AGE</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">coredns-9d85f5447-fksl4          0&#x2F;1     Pending   0          6m45s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">coredns-9d85f5447-qqkwn          0&#x2F;1     Pending   0          6m44s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">etcd-master                      1&#x2F;1     Running   0          7m2s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">kube-apiserver-master            1&#x2F;1     Running   0          7m2s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">kube-controller-manager-master   1&#x2F;1     Running   0          7m1s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">kube-proxy-h2kdx                 1&#x2F;1     Running   0          6m45s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">kube-scheduler-master            1&#x2F;1     Running   0          7m1s</span></pre></td></tr></table></figure><p>从输出中可以看到，coredns 的 Pod 处于 Pending 状态，即调度失败，当然也符合预期，因为这个 Master 节点的网络尚未就绪。</p><h1 id="部署网络插件"><a href="#部署网络插件" class="headerlink" title="部署网络插件"></a>部署网络插件</h1><p>本次我们选择 Weave 为网络插件，只需执行一句<code>kubectl apply</code>指令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl apply -f &quot;https:&#x2F;&#x2F;cloud.weave.works&#x2F;k8s&#x2F;net?k8s-version&#x3D;$(kubectl version | base64 | tr -d &#39;\n&#39;)&quot;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">serviceaccount&#x2F;weave-net created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">clusterrole.rbac.authorization.k8s.io&#x2F;weave-net created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;weave-net created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">role.rbac.authorization.k8s.io&#x2F;weave-net created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">rolebinding.rbac.authorization.k8s.io&#x2F;weave-net created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">daemonset.apps&#x2F;weave-net created</span></pre></td></tr></table></figure><p>部署完，等一两分钟，再次检查pod状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl get pods -n kube-system</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">NAME                             READY   STATUS    RESTARTS   AGE</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">coredns-9d85f5447-fksl4          1&#x2F;1     Running   0          12m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">coredns-9d85f5447-qqkwn          1&#x2F;1     Running   0          12m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">etcd-master                      1&#x2F;1     Running   0          12m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">kube-apiserver-master            1&#x2F;1     Running   0          12m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">kube-controller-manager-master   1&#x2F;1     Running   0          12m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">kube-proxy-h2kdx                 1&#x2F;1     Running   0          12m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">kube-scheduler-master            1&#x2F;1     Running   0          12m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">weave-net-xssqj                  2&#x2F;2     Running   0          95s</span></pre></td></tr></table></figure><p>现在，所有的系统 Pod 都成功启动了，Weave 插件新建了一个名叫 weave-net-xssqj 的 Pod，这就是容器网络插件在每个节点上的控制组件。</p><h1 id="部署-Kubernetes-的-Worker-节点"><a href="#部署-Kubernetes-的-Worker-节点" class="headerlink" title="部署 Kubernetes 的 Worker 节点"></a>部署 Kubernetes 的 Worker 节点</h1><p>Kubernetes 的 Worker 节点和 Master 节点几乎是相同的，它们运行的都是一个 kubelet 组件。区别在于，Master 节点上多运行这 kube-apiserver、kube-scheduler、kube-controller-manager 这三个系统 Pod。</p><p>部署 Worker 节点是最简单的，只需要两步即可。</p><p>第一步，在所有的 Worker 节点上执行“安装 Docker 和 kubeadm”这一步的操作</p><p>第二步，执行部署 Master 节点时生成的<code>kubeadm join</code>命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">kubeadm join 172.31.0.57:6443 --token 1octc9.3uvkgo5vy1sgr7vy \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    --discovery-token-ca-cert-hash sha256:72fc3bf1a6cc0a159a27bf99518c116845b6a8ae05fb8e78cddc4c7156777c10</span></pre></td></tr></table></figure><p>这时，我们在 Master 节点上查看一下当前集群的节点（node）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl get node</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">NAME       STATUS   ROLES    AGE   VERSION</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">master     Ready    master   20m   v1.17.1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">worker01   Ready    &lt;none&gt;   1m   v1.17.1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">worker02   Ready    &lt;none&gt;   1m   v1.17.1</span></pre></td></tr></table></figure><p>可以看到，当前集群有三个节点，并且都是 Ready。</p><h1 id="通过-Taint-调整-Master-执行-Pod-的策略"><a href="#通过-Taint-调整-Master-执行-Pod-的策略" class="headerlink" title="通过 Taint 调整 Master 执行 Pod 的策略"></a>通过 Taint 调整 Master 执行 Pod 的策略</h1><blockquote><p>备注：默认情况下，Master 节点是不允许运行用户的 Pod 的。</p></blockquote><p>原理比较简单：一旦某个节点被打上了一个 Taint，即“有了污点”，那么所有的 Pod 就都不能在这个节点上运行。</p><p>通过 <code>kubectl descirbe</code> 检查一下 Master 节点的 Taint 字段，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl describe node master</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Name:               master</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Roles:              master</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">Labels:             ......</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Annotations:        ......</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">CreationTimestamp:  ......</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">Taints:             node-role.kubernetes.io&#x2F;master:NoSchedule</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">......</span></pre></td></tr></table></figure><p>可以看到，Master节点默认加上了一个<code>node-role.kubernetes.io/master:NoSchedule</code>这样的Taint，键是<code>node-role.kubernetes.io/master</code>，值是<code>NoSchedule</code>。</p><p>所以，需要将这个Taint删掉才行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl taint nodes --all node-role.kubernetes.io&#x2F;master-</span></pre></td></tr></table></figure><p>我们在<code>node-role.kubernetes.io/master</code>这个键后面加上了一个短<br>横线<code>-</code>，这个格式就意味着移除所有以<code>node-role.kubernetes.io/master</code>为<br>键的 Taint。</p><p>至此，一个基本完整的 Kubernetes 集群就部署完毕了。</p><p>接下来，会再安装一些其他的辅助插件，如 Dashboard 和存储插件。</p><h1 id="部署-Dashboard-可视化插件（可选）"><a href="#部署-Dashboard-可视化插件（可选）" class="headerlink" title="部署 Dashboard 可视化插件（可选）"></a>部署 Dashboard 可视化插件（可选）</h1><p>Dashboard 的部署也很简单，还是通过<code>kubectl apply</code>命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes&#x2F;dashboard&#x2F;v2.0.0-rc2&#x2F;aio&#x2F;deploy&#x2F;recommended.yaml</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">root@master:~# mv recommended.yaml kubernetes-dashboard.yaml</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl apply -f kubernetes-dashboard.yaml</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">namespace&#x2F;kubernetes-dashboard created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">serviceaccount&#x2F;kubernetes-dashboard created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">service&#x2F;kubernetes-dashboard created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">secret&#x2F;kubernetes-dashboard-certs created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">secret&#x2F;kubernetes-dashboard-csrf created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">secret&#x2F;kubernetes-dashboard-key-holder created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">configmap&#x2F;kubernetes-dashboard-settings created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">role.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">clusterrole.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">rolebinding.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">deployment.apps&#x2F;kubernetes-dashboard created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">service&#x2F;dashboard-metrics-scraper created</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">deployment.apps&#x2F;dashboard-metrics-scraper created</span></pre></td></tr></table></figure><p>部署完成后，在kubernetes-dashboard的命名空间下，会新建两个 Pod。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl get pod -n kubernetes-dashboard</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">NAME                                         READY   STATUS    RESTARTS   AGE</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">dashboard-metrics-scraper-7b64584c5c-w46jk   1&#x2F;1     Running   0          3m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">kubernetes-dashboard-566f567dc7-554zh        1&#x2F;1     Running   0          3m</span></pre></td></tr></table></figure><blockquote><p>注意：Dashboard 部署完成后，默认只能通过 Proxy 的方式在本地访问。如果想从集群外访问这个 Dashboard 的话，需要用到 Ingress。</p></blockquote><h1 id="部署容器存储插件"><a href="#部署容器存储插件" class="headerlink" title="部署容器存储插件"></a>部署容器存储插件</h1><p>本次部署选择 Rook 存储插件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;rook&#x2F;rook&#x2F;master&#x2F;cluster&#x2F;examples&#x2F;kubernetes&#x2F;ceph&#x2F;common.yaml</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;rook&#x2F;rook&#x2F;master&#x2F;cluster&#x2F;examples&#x2F;kubernetes&#x2F;ceph&#x2F;operator.yaml</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;rook&#x2F;rook&#x2F;master&#x2F;cluster&#x2F;examples&#x2F;kubernetes&#x2F;ceph&#x2F;cluster.yaml</span></pre></td></tr></table></figure><p>部署完成后，可以看到 Rook 在rook-ceph的命名空间下新建了一些 Pod。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl get pod -n rook-ceph           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">NAME                                                 READY   STATUS      RESTARTS   AGE</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">csi-cephfsplugin-4t7qj                               3&#x2F;3     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">csi-cephfsplugin-9db7n                               3&#x2F;3     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">csi-cephfsplugin-pb8pv                               3&#x2F;3     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">csi-cephfsplugin-provisioner-8b9d48896-2qk22         4&#x2F;4     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">csi-cephfsplugin-provisioner-8b9d48896-dgbkm         4&#x2F;4     Running     1          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">csi-rbdplugin-4ln69                                  3&#x2F;3     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">csi-rbdplugin-7qp7d                                  3&#x2F;3     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">csi-rbdplugin-provisioner-6d465d6c6f-4jsfj           5&#x2F;5     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">csi-rbdplugin-provisioner-6d465d6c6f-w4gzx           5&#x2F;5     Running     1          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">csi-rbdplugin-sqmvz                                  3&#x2F;3     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">rook-ceph-crashcollector-master-6b8dd5d4fd-thq4w     1&#x2F;1     Running     0          3m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">rook-ceph-crashcollector-worker01-68d69757d7-j9fzs   1&#x2F;1     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">rook-ceph-crashcollector-worker02-8474bd88d-xftgn    1&#x2F;1     Running     0          3m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">rook-ceph-mgr-a-76c6c49cc9-7tlzb                     1&#x2F;1     Running     0          3m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">rook-ceph-mon-a-59998d787-bdvlv                      1&#x2F;1     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">rook-ceph-mon-b-6d69bddc95-vvvkq                     1&#x2F;1     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">rook-ceph-mon-c-776d969f6c-89fhm                     1&#x2F;1     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">rook-ceph-operator-678887c8d-sm8bq                   1&#x2F;1     Running     0          3m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">rook-ceph-osd-0-78fd8856df-lrfjx                     1&#x2F;1     Running     0          3m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">rook-ceph-osd-1-5cd4b9564f-k55xt                     1&#x2F;1     Running     0          3m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">rook-ceph-osd-2-7c4d99694f-j8phk                     1&#x2F;1     Running     0          3m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">rook-ceph-osd-prepare-master-8f69h                   0&#x2F;1     Completed   0          3m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">rook-ceph-osd-prepare-worker01-pm6cq                 0&#x2F;1     Completed   0          3m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">rook-ceph-osd-prepare-worker02-dg7gb                 0&#x2F;1     Completed   0          3m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">rook-discover-bpnc7                                  1&#x2F;1     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">rook-discover-bs9hj                                  1&#x2F;1     Running     0          4m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">rook-discover-wwvr2                                  1&#x2F;1     Running     0          4m</span></pre></td></tr></table></figure><p>rook-ceph-osd-prepare-master-8f69h、rook-ceph-osd-prepare-worker01-pm6cq 、rook-ceph-osd-prepare-worker02-dg7gb  它们的状态是 Completed，这个没有影响，其实它们是一种 job，完成部署 osd 的工作，执行完之后就变成 Completed 了。真正的 osd 的 Pod 是rook-ceph-osd-0-78fd8856df-lrfjx、rook-ceph-osd-1-5cd4b9564f-k55xt、rook-ceph-osd-2-7c4d99694f-j8phk。</p><p>另外，默认启动的Ceph集群，是开启Ceph认证的，这样你登陆Ceph组件所在的Pod里，是没法去获取集群状态，以及执行CLI命令，需要部署Ceph toolbox，配置文件如<code>toolbox.yaml</code>所示（参考官网）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">kind: Deployment</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">metadata:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  name: rook-ceph-tools</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  namespace: rook-ceph</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  labels:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    app: rook-ceph-tools</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">spec:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  replicas: 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  selector:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    matchLabels:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">      app: rook-ceph-tools</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">  template:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    metadata:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      labels:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        app: rook-ceph-tools</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    spec:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">      dnsPolicy: ClusterFirstWithHostNet</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">      containers:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">      - name: rook-ceph-tools</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">        image: rook&#x2F;ceph:v1.2.2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">        command: [&quot;&#x2F;tini&quot;]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        args: [&quot;-g&quot;, &quot;--&quot;, &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;toolbox.sh&quot;]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        imagePullPolicy: IfNotPresent</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">        env:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">          - name: ROOK_ADMIN_SECRET</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">            valueFrom:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">              secretKeyRef:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">                name: rook-ceph-mon</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">                key: admin-secret</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">        securityContext:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">          privileged: true</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">        volumeMounts:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">          - mountPath: &#x2F;dev</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">            name: dev</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">          - mountPath: &#x2F;sys&#x2F;bus</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">            name: sysbus</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">          - mountPath: &#x2F;lib&#x2F;modules</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">            name: libmodules</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">          - name: mon-endpoint-volume</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">            mountPath: &#x2F;etc&#x2F;rook</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">      # if hostNetwork: false, the &quot;rbd map&quot; command hangs, see https:&#x2F;&#x2F;github.com&#x2F;rook&#x2F;rook&#x2F;issues&#x2F;2021</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">      hostNetwork: true</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">      volumes:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">        - name: dev</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">          hostPath:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">            path: &#x2F;dev</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">        - name: sysbus</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">          hostPath:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">            path: &#x2F;sys&#x2F;bus</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">        - name: libmodules</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">          hostPath:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">            path: &#x2F;lib&#x2F;modules</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">        - name: mon-endpoint-volume</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">          configMap:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">            name: rook-ceph-mon-endpoints</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">            items:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">            - key: data</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">              path: mon-endpoints</span></pre></td></tr></table></figure><p>执行 rook-ceph-tools的 pod</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl apply -f toolbox.yaml</span></pre></td></tr></table></figure><p>部署完成后，可以看到在rook-ceph的命名空间下多了<code>rook-ceph-tools-856c5bc6b4-mw7t8</code> Pod</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl get pod -n rook-ceph| grep tools</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">rook-ceph-tools-856c5bc6b4-mw7t8                     1&#x2F;1     Running     0          3m</span></pre></td></tr></table></figure><p>现在，进入到pod当中，执行<code>ceph -s</code>，查看ceph 集群的状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@master:~# kubectl exec -it rook-ceph-tools-856c5bc6b4-mw7t8  -n rook-ceph bash</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[root@worker02 &#x2F;]# ceph -s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  cluster:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    id:     11436e29-c940-4a4f-9875-c65fac3531c1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    health: HEALTH_OK</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  services:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    mon: 3 daemons, quorum a,b,c (age 5h)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    mgr: a(active, since 5m)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    osd: 3 osds: 3 up (since 5m), 3 in (since 5m)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">  data:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    pools:   0 pools, 0 pgs</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    objects: 0 objects, 0 B</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">    usage:   3.0 GiB used, 294 GiB &#x2F; 297 GiB avail</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    pgs:</span></pre></td></tr></table></figure><p>可以看到，ceph集群是HEALTH_OK的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h1&gt;&lt;p&gt;本次部署一共三个节点，一个master，两个worker，都是 Ubuntu16.04 的虚拟机。&lt;/p&gt;
&lt;ul&gt;
&lt;li
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
