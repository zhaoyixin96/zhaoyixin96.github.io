{"meta":{"title":"ZhaoYixin's blog","subtitle":"Quick notes","description":"","author":"zhaoyixin","url":"https://zhaoyixin.xyz","root":"/"},"pages":[],"posts":[{"title":"使用VBD共享云硬盘搭建RHCS集群","slug":"使用VBD共享云硬盘搭建RHCS集群","date":"2020-02-21T06:35:49.000Z","updated":"2020-02-21T06:48:00.421Z","comments":true,"path":"2020/02/21/使用VBD共享云硬盘搭建RHCS集群/","link":"","permalink":"https://zhaoyixin.xyz/2020/02/21/%E4%BD%BF%E7%94%A8VBD%E5%85%B1%E4%BA%AB%E4%BA%91%E7%A1%AC%E7%9B%98%E6%90%AD%E5%BB%BARHCS%E9%9B%86%E7%BE%A4/","excerpt":"","text":"简介以操作系统为CentOS 6.5的云服务器为例，搭建RHCS（Red Hat Cluster Suite）集群系统，使用GFS2分布式文件系统，实现一个共享盘在多个云服务器之间可共享文件。 将共享云硬盘挂载给多台云服务器后，需要安装共享文件系统或类似的集群管理系统，才能实现在多台云服务器之间共享文件。 环境准备本次搭建一共三个节点，一个管理节点(ecs-share-003)，两个业务节点(ecs-share-001、ecs-share-002)，都是Centos6.5的云服务器。 一块VBD型的共享云硬盘 ecs-share-003:10.110.31.166 ecs-share-001:10.110.31.29 ecs-share-002:10.110.31.206 将共享云硬盘挂载到ecs-share-001和ecs-share-002两台云服务器上。 配置 1核CPU 2G内存 Centos6.5 内网互通 搭建流程 配置云服务器网络 安装RHCS集群 创建集群 配置磁盘 验证磁盘共享功能 配置云服务器网络本操作在三个节点都要执行，现以ecs-share-001为例 1[root@ecs-share-001 ~]# vi &#x2F;etc&#x2F;hosts23#在最后添加以下内容，保存后退出410.110.31.29 ecs-share-001510.110.31.206 ecs-share-002610.110.31.166 ecs-share-003 1[root@ecs-share-001 ~]# cat &#x2F;etc&#x2F;hosts2127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain43::1 localhost localhost.localdomain localhost6 localhost6.localdomain6410.110.31.29 ecs-share-001510.110.31.206 ecs-share-002610.110.31.166 ecs-share-003 安装RHCS集群为业务节点安装ricci软件，为管理节点安装luci软件。 luci。RHCS集群管理工具的主控端，提供了管理RHCS集群的web页面，管理集群主要是通过跟集群中其他节点上的ricci通信来完成的。 ricci。RHCS集群管理工具的受控端，安装在集群中的其他节点上，luci就是通过每一个节点上的ricci管理后端。 安装ricci本操作在ecs-share-001和ecs-share-002上来完成，现以ecs-share-001为例。 下载并安装ricci软件包 1[root@ecs-share-001 ~]# yum install ricci openais cman rgmanager lvm2-cluster gfs2-utils 关闭防火墙 1[root@ecs-share-001 ~]# &#x2F;etc&#x2F;init.d&#x2F;iptables stop2[root@ecs-share-001 ~]# chkconfig iptables off 暂停并关闭ACPI（Advanced Configuration and Power Interface）服务 1[root@ecs-share-001 ~]# &#x2F;etc&#x2F;init.d&#x2F;acpid stop2Stopping acpi daemon: [ OK ]3[root@ecs-share-001 ~]# chkconfig acpid off 设置ricci密码 1[root@ecs-share-001 ~]# passwd ricci2Changing password for user ricci.3New password: 4BAD PASSWORD: it is too short5BAD PASSWORD: is too simple6Retype new password: 7passwd: all authentication tokens updated successfully. 启动ricci 1[root@ecs-share-001 ~]# &#x2F;etc&#x2F;init.d&#x2F;ricci start2Starting system message bus: [ OK ]3Starting oddjobd: [ OK ]4generating SSL certificates... done5Generating NSS database... done6Starting ricci: [ OK ]7[root@ecs-share-001 ~]# chkconfig ricci on 查看云服务器挂载的磁盘 1[root@ecs-share-001 ~]# fdisk -l23Disk &#x2F;dev&#x2F;vda: 42.9 GB, 42949672960 bytes416 heads, 63 sectors&#x2F;track, 83220 cylinders5Units &#x3D; cylinders of 1008 * 512 &#x3D; 516096 bytes6Sector size (logical&#x2F;physical): 512 bytes &#x2F; 512 bytes7I&#x2F;O size (minimum&#x2F;optimal): 512 bytes &#x2F; 512 bytes8Disk identifier: 0x000342fb910 Device Boot Start End Blocks Id System11&#x2F;dev&#x2F;vda1 * 3 1043 524288 83 Linux12Partition 1 does not end on cylinder boundary.13&#x2F;dev&#x2F;vda2 1043 5204 2097152 82 Linux swap &#x2F; Solaris14Partition 2 does not end on cylinder boundary.15&#x2F;dev&#x2F;vda3 5204 83221 39320576 83 Linux16Partition 3 does not end on cylinder boundary.1718Disk &#x2F;dev&#x2F;vdb: 67 MB, 67108864 bytes1916 heads, 63 sectors&#x2F;track, 130 cylinders20Units &#x3D; cylinders of 1008 * 512 &#x3D; 516096 bytes21Sector size (logical&#x2F;physical): 512 bytes &#x2F; 512 bytes22I&#x2F;O size (minimum&#x2F;optimal): 512 bytes &#x2F; 512 bytes23Disk identifier: 0x000000002425 Device Boot Start End Blocks Id System2627Disk &#x2F;dev&#x2F;vdc: 21.5 GB, 21474836480 bytes2816 heads, 63 sectors&#x2F;track, 41610 cylinders29Units &#x3D; cylinders of 1008 * 512 &#x3D; 516096 bytes30Sector size (logical&#x2F;physical): 512 bytes &#x2F; 512 bytes31I&#x2F;O size (minimum&#x2F;optimal): 512 bytes &#x2F; 512 bytes32Disk identifier: 0x00000000 新建磁盘挂载目录 1[root@ecs-share-001 ~]# mkdir &#x2F;mnt&#x2F;gfs_vbd 为磁盘创建GFS2分布式文件系统 1[root@ecs-share-001 ~]# mkfs.gfs2 -p lock_dlm -t mycluster:gfs_vbd -j 4 &#x2F;dev&#x2F;vdc2This will destroy any data on &#x2F;dev&#x2F;vdc.3It appears to contain: data45Are you sure you want to proceed? [y&#x2F;n] y67Device: &#x2F;dev&#x2F;vdc8Blocksize: 40969Device Size 20.00 GB (5242880 blocks)10Filesystem Size: 20.00 GB (5242878 blocks)11Journals: 412Resource Groups: 8013Locking Protocol: &quot;lock_dlm&quot;14Lock Table: &quot;mycluster:gfs_vbd&quot;15UUID: 868002c6-ede7-fc8f-d8b6-b2df3d42bae1 安装luci本操作在管理节点ecs-share-003上完成。 下载并安装luci集群软件包 1[root@ecs-share-003 ~]# yum install luci 关闭防火墙 1[root@ecs-share-003 ~]# &#x2F;etc&#x2F;init.d&#x2F;iptables stop2[root@ecs-share-003 ~]# chkconfig iptables off 启动luci 1[root@ecs-share-003 ~]# &#x2F;etc&#x2F;init.d&#x2F;luci start2Adding following auto-detected host IDs (IP addresses&#x2F;domain names), corresponding to &#96;ecs-share-003&#39; address, to the configuration of self-managed certificate &#96;&#x2F;var&#x2F;lib&#x2F;luci&#x2F;etc&#x2F;cacert.config&#39; (you can change them by editing &#96;&#x2F;var&#x2F;lib&#x2F;luci&#x2F;etc&#x2F;cacert.config&#39;, removing the generated certificate &#96;&#x2F;var&#x2F;lib&#x2F;luci&#x2F;certs&#x2F;host.pem&#39; and restarting luci):3 (none suitable found, you can still do it manually as mentioned above)45Generating a 2048 bit RSA private key6writing new private key to &#39;&#x2F;var&#x2F;lib&#x2F;luci&#x2F;certs&#x2F;host.pem&#39;7Starting saslauthd: [ OK ]8Start luci... [ OK ]9Point your web browser to https:&#x2F;&#x2F;ecs-share-003:8084 (or equivalent) to access luci1011[root@ecs-share-003 ~]# chkconfig luci on luci启动成功后，即可以通过管理RHCS集群的web页面来进行集群的相关配置。 创建集群打开并登录管理web页面 地址：https://10.110.31.166:8084 账号密码：登录云服务的root账号和密码 选择”Manage Cluster”，点击”Create”创建集群 Cluser Name：自定义集群名称，例如，mycluster。 Node Name：分别设置业务节点的名称，例如，ecs-share-0001。 Paasword：安装ricci时设置的ricci密码。 添加完第一个节点的信息后，单击”Add another Node”添加第二个节点。具体参数如 图1所示。 图1 创建集群 配置磁盘创建成功后，选择”Reources”，单击”Add”为集群新建磁盘资源。 Name：自定义磁盘资源名称，例如，GFS_VBD。 Mount Point：安装ricci中设置的磁盘挂载目录，例如，/mnt/gfs_vbd。 Device，FS Label，or UUID：此处以填写安装ricci中磁盘设备名称为例，例如，/dev/vdc。 Filesystem Type：此处选择安装ricci中设置的分布式文件系统，例如，GFS2。 具体参数如 图2所示。 图2 创建磁盘资源 创建成功后，如 图3所示。 图3 创建磁盘资源成功 然后，选择”Failover Domains”，单击”Add”为集群服务新建故障切换域。 本例添加了名称为gfs_failover的故障切换域，具体参数如 图4所示。 图4 创建故障切换域 创建成功后再选择”Service Groups”，单击”Add”为集群中的节点创建服务组。 Service Name：自定义服务组的名称，例如，GFS_SG_VBD_1。 Failover Domain：选择创建的故障切换域，例如，gfs_failover。 Add Resource：选择创建的资源，例如，GFS_VBD。 本示例添加了名称为GFS_SG_VBD_1的服务组，并添加了GFS_VBD磁盘资源，具体参数如 图5和 图6所示。 图5 创建服务组 图6 添加GFS_VBD磁盘资源 单击”submit”后，选择start on ecs-share-001节点。按同样的操作也为ecs-share-002创建服务组。两个服务组都创建成功后，如 图7 所示。 图7 创建两个服务组成功 服务组创建完成后，分别查看云服务器ecs-share-001和ecs-share-002的集群状况。以ecs-share-001为例。 1[root@ecs-share-001 ~]# clustat2Cluster Status for mycluster @ Wed Feb 19 04:19:41 20203Member Status: Quorate45 Member Name ID Status6 ------ ---- ---- ------7 ecs-share-001 1 Online, Local, rgmanager8 ecs-share-002 2 Online, rgmanager910 Service Name Owner (Last) State 11 ------- ---- ----- ------ ----- 12 service:GFS_SG_VBD_1 ecs-share-001 started 13 service:GFS_SG_VBD_2 ecs-share-002 started 查看磁盘分区及挂载信息。 1[root@ecs-share-001 ~]# df -h2Filesystem Size Used Avail Use% Mounted on3&#x2F;dev&#x2F;vda3 37G 1.3G 34G 4% &#x2F;4tmpfs 939M 32M 908M 4% &#x2F;dev&#x2F;shm5&#x2F;dev&#x2F;vda1 504M 62M 417M 13% &#x2F;boot6&#x2F;dev&#x2F;vdc 20G 518M 20G 3% &#x2F;mnt&#x2F;gfs_vbd 其中，“/dev/vdc”即为共享盘设备名，分区成功并已挂载至“/mnt/gfs_vbd”目录下。 注意：如果执行完df -h命令发现分区表里没有挂载的共享盘，请执行“mount 磁盘设备名称 挂载目录” 命令重新挂载磁盘，例如，mount /dev/vdc /mnt/gfs_vbd，挂载后便可与其他云服务器同步使用该共享盘。 验证磁盘共享功能ecs-share-001写入内容使用root用户登录云服务器ecs-share-001，进入“/mnt/gfs_vbd/”目录。 1[root@ecs-share-001 ~]# cd &#x2F;mnt&#x2F;gfs_vbd&#x2F; 新建“testshare”文件，并写入以下内容。 1[root@ecs-share-001 gfs_vbd]# vi testshare2001 write34[root@ecs-share-001 gfs_vbd]# cat testshare 5001 write ecs-share-002查看内容，然后修改内容使用root用户登录云服务器ecs-share-002。验证在ecs-share-002的磁盘挂载目录“/mnt/gfs_vbd”能否看到“testshare”文件中的内容 1[root@ecs-share-002 ~]# cd &#x2F;mnt&#x2F;gfs_vbd&#x2F;2[root@ecs-share-002 gfs_vbd]# ls3testshare4[root@ecs-share-002 gfs_vbd]# cat testshare 5001 write 从显示结果中，可以看到内容同步成功。 然后修改testshare文件，写入以下内容 1[root@ecs-share-002 gfs_vbd]# vi testshare2#在最后添加以下内容，保存后退出3002 write45[root@ecs-share-002 gfs_vbd]# cat testshare 6001 write7002 write ecs-share-001查看内容改动使用root用户登录云服务器ecs-share-001，查看testshare文件中的内容 1[root@ecs-share-001 gfs_vbd]# cat testshare 2001 write3002 write 可以看到，内容修改同步成功。","categories":[],"tags":[]},{"title":"使用kubeadm搭建一个完整的kubernetes集群","slug":"使用kubeadm搭建一个完整的kubernetes集群","date":"2020-01-20T08:07:04.000Z","updated":"2020-02-21T06:44:45.693Z","comments":true,"path":"2020/01/20/使用kubeadm搭建一个完整的kubernetes集群/","link":"","permalink":"https://zhaoyixin.xyz/2020/01/20/%E4%BD%BF%E7%94%A8kubeadm%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84kubernetes%E9%9B%86%E7%BE%A4/","excerpt":"","text":"环境准备本次部署一共三个节点，一个master，两个worker，都是 Ubuntu16.04 的虚拟机。 master ： 172.31.0.57 worker01 ： 172.31.0.32 worker02 ： 172.31.0.53 配置 2核CPU 8G内存 40G系统盘+100G数据盘（数据盘用作ceph osd） Ubuntu16.04 内网互通 部署流程 在所有节点安装 Docker 和 kubeadm 部署 Kubernetes Master 部署容器网络插件 部署 Kubernetes Worker 通过 Taint 调整 Master 执行 Pod 的策略 部署 Dashboard 可视化插件（可选） 部署容器存储插件 安装 Docker 和 kubeadm 备注：本次部署都是在root用户下进行操作 这一步的所有操作在所有节点上都要执行 1root@master:~# apt-get update &amp;&amp; apt-get install -y apt-transport-https2root@master:~# curl https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt&#x2F;doc&#x2F;apt-key.gpg | apt-key add -3root@master:~# vi &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;kubernetes.list4# 添加以下内容，然后保存退出，使用阿里的镜像源5deb https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt&#x2F; kubernetes-xenial main6root@master:~# apt update7root@master:~# apt -y install docker.io kubeadm 在安装 kubeadm 的过程中，kubeadm 和 kubelet、kubectl、kubernetes-cni 这几个二进制文件都会被安装好。 安装 docker 直接使用 docker.io 的安装源，因为发布的最新的 Docker CE（社区版）往往没有经过 Kubernetes 项目的验证，可能会有兼容性问题。 另外，后续在执行 kubeadm 命令的时候，会进行一系列的检查工作（“preflight”），其中需要禁用虚拟内存（swap） 1root@master:~# swapoff -a2root@master:~# sed &#39;s&#x2F;.*swap.*&#x2F;#&amp;&#x2F;&#39; &#x2F;etc&#x2F;fstab 部署 Kubernetes 的 Master 节点首先查看一下我们本次部署的 kubernetes 的版本信息 1root@master:~# kubeadm config print init-defaults2.......3.......4apiServer:5 timeoutForControlPlane: 4m0s6apiVersion: kubeadm.k8s.io&#x2F;v1beta27certificatesDir: &#x2F;etc&#x2F;kubernetes&#x2F;pki8clusterName: kubernetes9controllerManager: &#123;&#125;10dns:11 type: CoreDNS12etcd:13 local:14 dataDir: &#x2F;var&#x2F;lib&#x2F;etcd15imageRepository: k8s.gcr.io16kind: ClusterConfiguration17kubernetesVersion: v1.17.018networking:19 dnsDomain: cluster.local20 serviceSubnet: 10.96.0.0&#x2F;1221scheduler: &#123;&#125; 备注：通过配置文件来开启一些实验性功能。 有了这些信息后，接下来编写一个给 kubeadm 用的 YAML 文件（kubeadm.yaml）: 1root@master:~# vi kubeadm.yaml23apiVersion: kubeadm.k8s.io&#x2F;v1beta24imageRepository: registry.aliyuncs.com&#x2F;google_containers5kind: ClusterConfiguration6controllerManager:7 extraArgs:8 horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot;9 horizontal-pod-autoscaler-sync-period: &quot;10s&quot;10 node-monitor-grace-period: &quot;10s&quot;11apiServer:12 extraArgs:13 runtime-config: &quot;api&#x2F;all&#x3D;true&quot;14kubernetesVersion: v1.17.0 apiVersion、kind、kubernetesVersion 都可以从上面 print 的返回中得到。 另外把 imageRepository 配置为 registry.aliyuncs.com/google_containers 阿里的源，因为默认的 k8s.gcr.io 因为一些原因访问不了。 horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot; 意味着，将来部署的 kube-controller-manager 能够使用自定义资源进行自动水平扩展。 接下来，只需要一句指令，就可以完成 Kubernetes 的部署 1root@master:~# kubeadm init --config kubeadm.yaml23......4......56Your Kubernetes control-plane has initialized successfully!78To start using your cluster, you need to run the following as a regular user:910 mkdir -p $HOME&#x2F;.kube11 sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config12 sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config1314You should now deploy a pod network to the cluster.15Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:16 https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;addons&#x2F;1718Then you can join any number of worker nodes by running the following on each as root:1920kubeadm join 172.31.0.57:6443 --token 1octc9.3uvkgo5vy1sgr7vy \\21 --discovery-token-ca-cert-hash sha256:72fc3bf1a6cc0a159a27bf99518c116845b6a8ae05fb8e78cddc4c7156777c10 部署完成后，kubeadm 会生成一行指令： 1kubeadm join 172.31.0.57:6443 --token 1octc9.3uvkgo5vy1sgr7vy \\2 --discovery-token-ca-cert-hash sha256:72fc3bf1a6cc0a159a27bf99518c116845b6a8ae05fb8e78cddc4c7156777c10 这个命令，就是用来给这个Master节点添加更多的工作节点使用的。 另外，kubeadm 还会提示我们一些配置命令，也要执行一下： 1root@master:~# mkdir -p $HOME&#x2F;.kube2root@master:~# cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config3root@master:~# chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config 这样配置的原因是：Kubernetes 集群默认需要加密方式访问。所以将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。 现在，用 kubectl get 命令查看当前唯一一个节点的状态： 1root@master:~# kubectl get nodes2NAME STATUS ROLES AGE VERSION3master NotReady master 2m26s v1.17.1 输出结果中，master 的状态是 NotReady，排查问题，最重要的手段就是用 kubectl describe 来查看这个节点（Node）对象的详细信息、状态和事件（Event）。 1root@master:~# kubectl describe node master23.....4Conditions:5 Type Status LastHeartbeatTime LastTransitionTime Reason Message6 ---- ------ ----------------- ------------------ ------ -------7 MemoryPressure False Sun, 19 Jan 2020 11:07:23 +0800 Sun, 19 Jan 2020 11:07:23 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available8 DiskPressure False Sun, 19 Jan 2020 11:07:23 +0800 Sun, 19 Jan 2020 11:07:23 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure9 PIDPressure False Sun, 19 Jan 2020 11:07:23 +0800 Sun, 19 Jan 2020 11:07:23 +0800 KubeletHasSufficientPID kubelet has sufficient PID available10 Ready False Sun, 19 Jan 2020 11:07:23 +0800 Sun, 19 Jan 2020 11:07:23 +0800 KubeletNotReady runtime network not ready: NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized1112..... 从输出中，可以看到原因在于为部署任何网络插件，network plugin is not ready: cni config uninitialized。 我们还可以通过 kubectl 查看这个节点上各个系统 Pod 的状态，其中 kube-system 是 Kubernetes 项目预留给系统 Pod 的工作空间。 1root@master:~# kubectl get pods -n kube-system 2NAME READY STATUS RESTARTS AGE3coredns-9d85f5447-fksl4 0&#x2F;1 Pending 0 6m45s4coredns-9d85f5447-qqkwn 0&#x2F;1 Pending 0 6m44s5etcd-master 1&#x2F;1 Running 0 7m2s6kube-apiserver-master 1&#x2F;1 Running 0 7m2s7kube-controller-manager-master 1&#x2F;1 Running 0 7m1s8kube-proxy-h2kdx 1&#x2F;1 Running 0 6m45s9kube-scheduler-master 1&#x2F;1 Running 0 7m1s 从输出中可以看到，coredns 的 Pod 处于 Pending 状态，即调度失败，当然也符合预期，因为这个 Master 节点的网络尚未就绪。 部署网络插件本次我们选择 Weave 为网络插件，只需执行一句kubectl apply指令 1root@master:~# kubectl apply -f &quot;https:&#x2F;&#x2F;cloud.weave.works&#x2F;k8s&#x2F;net?k8s-version&#x3D;$(kubectl version | base64 | tr -d &#39;\\n&#39;)&quot;2serviceaccount&#x2F;weave-net created3clusterrole.rbac.authorization.k8s.io&#x2F;weave-net created4clusterrolebinding.rbac.authorization.k8s.io&#x2F;weave-net created5role.rbac.authorization.k8s.io&#x2F;weave-net created6rolebinding.rbac.authorization.k8s.io&#x2F;weave-net created7daemonset.apps&#x2F;weave-net created 部署完，等一两分钟，再次检查pod状态 1root@master:~# kubectl get pods -n kube-system2NAME READY STATUS RESTARTS AGE3coredns-9d85f5447-fksl4 1&#x2F;1 Running 0 12m4coredns-9d85f5447-qqkwn 1&#x2F;1 Running 0 12m5etcd-master 1&#x2F;1 Running 0 12m6kube-apiserver-master 1&#x2F;1 Running 0 12m7kube-controller-manager-master 1&#x2F;1 Running 0 12m8kube-proxy-h2kdx 1&#x2F;1 Running 0 12m9kube-scheduler-master 1&#x2F;1 Running 0 12m10weave-net-xssqj 2&#x2F;2 Running 0 95s 现在，所有的系统 Pod 都成功启动了，Weave 插件新建了一个名叫 weave-net-xssqj 的 Pod，这就是容器网络插件在每个节点上的控制组件。 部署 Kubernetes 的 Worker 节点Kubernetes 的 Worker 节点和 Master 节点几乎是相同的，它们运行的都是一个 kubelet 组件。区别在于，Master 节点上多运行这 kube-apiserver、kube-scheduler、kube-controller-manager 这三个系统 Pod。 部署 Worker 节点是最简单的，只需要两步即可。 第一步，在所有的 Worker 节点上执行“安装 Docker 和 kubeadm”这一步的操作 第二步，执行部署 Master 节点时生成的kubeadm join命令 1kubeadm join 172.31.0.57:6443 --token 1octc9.3uvkgo5vy1sgr7vy \\2 --discovery-token-ca-cert-hash sha256:72fc3bf1a6cc0a159a27bf99518c116845b6a8ae05fb8e78cddc4c7156777c10 这时，我们在 Master 节点上查看一下当前集群的节点（node） 1root@master:~# kubectl get node2NAME STATUS ROLES AGE VERSION3master Ready master 20m v1.17.14worker01 Ready &lt;none&gt; 1m v1.17.15worker02 Ready &lt;none&gt; 1m v1.17.1 可以看到，当前集群有三个节点，并且都是 Ready。 通过 Taint 调整 Master 执行 Pod 的策略 备注：默认情况下，Master 节点是不允许运行用户的 Pod 的。 原理比较简单：一旦某个节点被打上了一个 Taint，即“有了污点”，那么所有的 Pod 就都不能在这个节点上运行。 通过 kubectl descirbe 检查一下 Master 节点的 Taint 字段， 1root@master:~# kubectl describe node master2Name: master3Roles: master4Labels: ......5Annotations: ......6CreationTimestamp: ......7Taints: node-role.kubernetes.io&#x2F;master:NoSchedule8...... 可以看到，Master节点默认加上了一个node-role.kubernetes.io/master:NoSchedule这样的Taint，键是node-role.kubernetes.io/master，值是NoSchedule。 所以，需要将这个Taint删掉才行 1root@master:~# kubectl taint nodes --all node-role.kubernetes.io&#x2F;master- 我们在node-role.kubernetes.io/master这个键后面加上了一个短横线-，这个格式就意味着移除所有以node-role.kubernetes.io/master为键的 Taint。 至此，一个基本完整的 Kubernetes 集群就部署完毕了。 接下来，会再安装一些其他的辅助插件，如 Dashboard 和存储插件。 部署 Dashboard 可视化插件（可选）Dashboard 的部署也很简单，还是通过kubectl apply命令 1root@master:~# wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes&#x2F;dashboard&#x2F;v2.0.0-rc2&#x2F;aio&#x2F;deploy&#x2F;recommended.yaml2root@master:~# mv recommended.yaml kubernetes-dashboard.yaml3root@master:~# kubectl apply -f kubernetes-dashboard.yaml4namespace&#x2F;kubernetes-dashboard created5serviceaccount&#x2F;kubernetes-dashboard created6service&#x2F;kubernetes-dashboard created7secret&#x2F;kubernetes-dashboard-certs created8secret&#x2F;kubernetes-dashboard-csrf created9secret&#x2F;kubernetes-dashboard-key-holder created10configmap&#x2F;kubernetes-dashboard-settings created11role.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard created12clusterrole.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard created13rolebinding.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard created14clusterrolebinding.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard created15deployment.apps&#x2F;kubernetes-dashboard created16service&#x2F;dashboard-metrics-scraper created17deployment.apps&#x2F;dashboard-metrics-scraper created 部署完成后，在kubernetes-dashboard的命名空间下，会新建两个 Pod。 1root@master:~# kubectl get pod -n kubernetes-dashboard2NAME READY STATUS RESTARTS AGE3dashboard-metrics-scraper-7b64584c5c-w46jk 1&#x2F;1 Running 0 3m4kubernetes-dashboard-566f567dc7-554zh 1&#x2F;1 Running 0 3m 注意：Dashboard 部署完成后，默认只能通过 Proxy 的方式在本地访问。如果想从集群外访问这个 Dashboard 的话，需要用到 Ingress。 部署容器存储插件本次部署选择 Rook 存储插件。 1root@master:~# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;rook&#x2F;rook&#x2F;master&#x2F;cluster&#x2F;examples&#x2F;kubernetes&#x2F;ceph&#x2F;common.yaml2root@master:~# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;rook&#x2F;rook&#x2F;master&#x2F;cluster&#x2F;examples&#x2F;kubernetes&#x2F;ceph&#x2F;operator.yaml3root@master:~# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;rook&#x2F;rook&#x2F;master&#x2F;cluster&#x2F;examples&#x2F;kubernetes&#x2F;ceph&#x2F;cluster.yaml 部署完成后，可以看到 Rook 在rook-ceph的命名空间下新建了一些 Pod。 1root@master:~# kubectl get pod -n rook-ceph 2NAME READY STATUS RESTARTS AGE3csi-cephfsplugin-4t7qj 3&#x2F;3 Running 0 4m4csi-cephfsplugin-9db7n 3&#x2F;3 Running 0 4m5csi-cephfsplugin-pb8pv 3&#x2F;3 Running 0 4m6csi-cephfsplugin-provisioner-8b9d48896-2qk22 4&#x2F;4 Running 0 4m7csi-cephfsplugin-provisioner-8b9d48896-dgbkm 4&#x2F;4 Running 1 4m8csi-rbdplugin-4ln69 3&#x2F;3 Running 0 4m9csi-rbdplugin-7qp7d 3&#x2F;3 Running 0 4m10csi-rbdplugin-provisioner-6d465d6c6f-4jsfj 5&#x2F;5 Running 0 4m11csi-rbdplugin-provisioner-6d465d6c6f-w4gzx 5&#x2F;5 Running 1 4m12csi-rbdplugin-sqmvz 3&#x2F;3 Running 0 4m13rook-ceph-crashcollector-master-6b8dd5d4fd-thq4w 1&#x2F;1 Running 0 3m14rook-ceph-crashcollector-worker01-68d69757d7-j9fzs 1&#x2F;1 Running 0 4m15rook-ceph-crashcollector-worker02-8474bd88d-xftgn 1&#x2F;1 Running 0 3m16rook-ceph-mgr-a-76c6c49cc9-7tlzb 1&#x2F;1 Running 0 3m17rook-ceph-mon-a-59998d787-bdvlv 1&#x2F;1 Running 0 4m18rook-ceph-mon-b-6d69bddc95-vvvkq 1&#x2F;1 Running 0 4m19rook-ceph-mon-c-776d969f6c-89fhm 1&#x2F;1 Running 0 4m20rook-ceph-operator-678887c8d-sm8bq 1&#x2F;1 Running 0 3m21rook-ceph-osd-0-78fd8856df-lrfjx 1&#x2F;1 Running 0 3m22rook-ceph-osd-1-5cd4b9564f-k55xt 1&#x2F;1 Running 0 3m23rook-ceph-osd-2-7c4d99694f-j8phk 1&#x2F;1 Running 0 3m24rook-ceph-osd-prepare-master-8f69h 0&#x2F;1 Completed 0 3m25rook-ceph-osd-prepare-worker01-pm6cq 0&#x2F;1 Completed 0 3m26rook-ceph-osd-prepare-worker02-dg7gb 0&#x2F;1 Completed 0 3m27rook-discover-bpnc7 1&#x2F;1 Running 0 4m28rook-discover-bs9hj 1&#x2F;1 Running 0 4m29rook-discover-wwvr2 1&#x2F;1 Running 0 4m rook-ceph-osd-prepare-master-8f69h、rook-ceph-osd-prepare-worker01-pm6cq 、rook-ceph-osd-prepare-worker02-dg7gb 它们的状态是 Completed，这个没有影响，其实它们是一种 job，完成部署 osd 的工作，执行完之后就变成 Completed 了。真正的 osd 的 Pod 是rook-ceph-osd-0-78fd8856df-lrfjx、rook-ceph-osd-1-5cd4b9564f-k55xt、rook-ceph-osd-2-7c4d99694f-j8phk。 另外，默认启动的Ceph集群，是开启Ceph认证的，这样你登陆Ceph组件所在的Pod里，是没法去获取集群状态，以及执行CLI命令，需要部署Ceph toolbox，配置文件如toolbox.yaml所示（参考官网）： 1apiVersion: apps&#x2F;v12kind: Deployment3metadata:4 name: rook-ceph-tools5 namespace: rook-ceph6 labels:7 app: rook-ceph-tools8spec:9 replicas: 110 selector:11 matchLabels:12 app: rook-ceph-tools13 template:14 metadata:15 labels:16 app: rook-ceph-tools17 spec:18 dnsPolicy: ClusterFirstWithHostNet19 containers:20 - name: rook-ceph-tools21 image: rook&#x2F;ceph:v1.2.222 command: [&quot;&#x2F;tini&quot;]23 args: [&quot;-g&quot;, &quot;--&quot;, &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;toolbox.sh&quot;]24 imagePullPolicy: IfNotPresent25 env:26 - name: ROOK_ADMIN_SECRET27 valueFrom:28 secretKeyRef:29 name: rook-ceph-mon30 key: admin-secret31 securityContext:32 privileged: true33 volumeMounts:34 - mountPath: &#x2F;dev35 name: dev36 - mountPath: &#x2F;sys&#x2F;bus37 name: sysbus38 - mountPath: &#x2F;lib&#x2F;modules39 name: libmodules40 - name: mon-endpoint-volume41 mountPath: &#x2F;etc&#x2F;rook42 # if hostNetwork: false, the &quot;rbd map&quot; command hangs, see https:&#x2F;&#x2F;github.com&#x2F;rook&#x2F;rook&#x2F;issues&#x2F;202143 hostNetwork: true44 volumes:45 - name: dev46 hostPath:47 path: &#x2F;dev48 - name: sysbus49 hostPath:50 path: &#x2F;sys&#x2F;bus51 - name: libmodules52 hostPath:53 path: &#x2F;lib&#x2F;modules54 - name: mon-endpoint-volume55 configMap:56 name: rook-ceph-mon-endpoints57 items:58 - key: data59 path: mon-endpoints 执行 rook-ceph-tools的 pod 1root@master:~# kubectl apply -f toolbox.yaml 部署完成后，可以看到在rook-ceph的命名空间下多了rook-ceph-tools-856c5bc6b4-mw7t8 Pod 1root@master:~# kubectl get pod -n rook-ceph| grep tools2rook-ceph-tools-856c5bc6b4-mw7t8 1&#x2F;1 Running 0 3m 现在，进入到pod当中，执行ceph -s，查看ceph 集群的状态 1root@master:~# kubectl exec -it rook-ceph-tools-856c5bc6b4-mw7t8 -n rook-ceph bash23[root@worker02 &#x2F;]# ceph -s4 cluster:5 id: 11436e29-c940-4a4f-9875-c65fac3531c16 health: HEALTH_OK7 8 services:9 mon: 3 daemons, quorum a,b,c (age 5h)10 mgr: a(active, since 5m)11 osd: 3 osds: 3 up (since 5m), 3 in (since 5m)12 13 data:14 pools: 0 pools, 0 pgs15 objects: 0 objects, 0 B16 usage: 3.0 GiB used, 294 GiB &#x2F; 297 GiB avail17 pgs: 可以看到，ceph集群是HEALTH_OK的。","categories":[],"tags":[]}]}