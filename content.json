{"meta":{"title":"ZhaoYixin's blog","subtitle":"Quick notes","description":"","author":"zhaoyixin","url":"https://zhaoyixin.xyz","root":"/"},"pages":[],"posts":[{"title":"使用kubeadm搭建一个完整的kubernetes集群","slug":"使用kubeadm搭建一个完整的kubernetes集群","date":"2020-01-20T08:07:04.000Z","updated":"2020-01-20T08:09:39.397Z","comments":true,"path":"2020/01/20/使用kubeadm搭建一个完整的kubernetes集群/","link":"","permalink":"https://zhaoyixin.xyz/2020/01/20/%E4%BD%BF%E7%94%A8kubeadm%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84kubernetes%E9%9B%86%E7%BE%A4/","excerpt":"","text":"环境准备本次部署一共三个节点，一个master，两个worker，都是 Ubuntu16.04 的虚拟机。 master ： 172.31.0.57 worker01 ： 172.31.0.32 worker02 ： 172.31.0.53 配置 2核CPU 8G内存 40G系统盘+100G数据盘（数据盘用作ceph osd） Ubuntu16.04 内网互通 部署流程 在所有节点安装 Docker 和 kubeadm 部署 Kubernetes Master 部署容器网络插件 部署 Kubernetes Worker 通过 Taint 调整 Master 执行 Pod 的策略 部署 Dashboard 可视化插件（可选） 部署容器存储插件 1.安装 Docker 和 kubeadm 备注：本次部署都是在root用户下进行操作 1root@master:~# apt-get update &amp;&amp; apt-get install -y apt-transport-https2root@master:~# curl https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt&#x2F;doc&#x2F;apt-key.gpg | apt-key add -3root@master:~# vi &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;kubernetes.list4# 添加以下内容，然后保存退出，使用阿里的镜像源5deb https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt&#x2F; kubernetes-xenial main6root@master:~# apt update7root@master:~# apt -y install docker.io kubeadm 在安装 kubeadm 的过程中，kubeadm 和 kubelet、kubectl、kubernetes-cni 这几个二进制文件都会被安装好。 安装 docker 直接使用 docker.io 的安装源，因为发布的最新的 Docker CE（社区版）往往没有经过 Kubernetes 项目的验证，可能会有兼容性问题。 2.部署 Kubernetes 的 Master 节点 备注：通过配置文件来开启一些实验性功能。 首先查看一下我们本次部署的 kubernetes 的版本信息 1root@master:~# kubeadm config print init-defaults2W0119 10:46:32.946635 7648 validation.go:28] Cannot validate kube-proxy config - no validator is available3W0119 10:46:32.946715 7648 validation.go:28] Cannot validate kubelet config - no validator is available4apiVersion: kubeadm.k8s.io&#x2F;v1beta25bootstrapTokens:6- groups:7 - system:bootstrappers:kubeadm:default-node-token8 token: abcdef.0123456789abcdef9 ttl: 24h0m0s10 usages:11 - signing12 - authentication13kind: InitConfiguration14localAPIEndpoint:15 advertiseAddress: 1.2.3.416 bindPort: 644317nodeRegistration:18 criSocket: &#x2F;var&#x2F;run&#x2F;dockershim.sock19 name: master20 taints:21 - effect: NoSchedule22 key: node-role.kubernetes.io&#x2F;master23---24apiServer:25 timeoutForControlPlane: 4m0s26apiVersion: kubeadm.k8s.io&#x2F;v1beta227certificatesDir: &#x2F;etc&#x2F;kubernetes&#x2F;pki28clusterName: kubernetes29controllerManager: &#123;&#125;30dns:31 type: CoreDNS32etcd:33 local:34 dataDir: &#x2F;var&#x2F;lib&#x2F;etcd35imageRepository: k8s.gcr.io36kind: ClusterConfiguration37kubernetesVersion: v1.17.038networking:39 dnsDomain: cluster.local40 serviceSubnet: 10.96.0.0&#x2F;1241scheduler: &#123;&#125; 有了这些信息后，接下来编写一个给 kubeadm 用的 YAML 文件（kubeadm.yaml）: 1root@master:~# vi kubeadm.yaml23apiVersion: kubeadm.k8s.io&#x2F;v1beta24imageRepository: registry.aliyuncs.com&#x2F;google_containers5kind: ClusterConfiguration6controllerManager:7 extraArgs:8 horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot;9 horizontal-pod-autoscaler-sync-period: &quot;10s&quot;10 node-monitor-grace-period: &quot;10s&quot;11apiServer:12 extraArgs:13 runtime-config: &quot;api&#x2F;all&#x3D;true&quot;14kubernetesVersion: v1.17.0 apiVersion、kind、kubernetesVersion 都可以从上面 print 的返回中得到。 另外把 imageRepository 配置为 registry.aliyuncs.com/google_containers 阿里的源，因为默认的 k8s.gcr.io 因为一些原因访问不了。 horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot; 意味着，将来部署的 kube-controller-manager 能够使用自定义资源进行自动水平扩展。 接下来，只需要一句指令，就可以完成 Kubernetes 的部署 1root@master:~# kubeadm init --config kubeadm.yaml2W0119 11:03:23.675227 10036 validation.go:28] Cannot validate kube-proxy config - no validator is available3W0119 11:03:23.675318 10036 validation.go:28] Cannot validate kubelet config - no validator is available4[init] Using Kubernetes version: v1.17.05[preflight] Running pre-flight checks6 [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;setup&#x2F;cri&#x2F;7 [WARNING Hostname]: hostname &quot;master&quot; could not be reached8 [WARNING Hostname]: hostname &quot;master&quot;: lookup master on 114.114.114.114:53: no such host9[preflight] Pulling images required for setting up a Kubernetes cluster10[preflight] This might take a minute or two, depending on the speed of your internet connection11[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;12[kubelet-start] Writing kubelet environment file with flags to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env&quot;13[kubelet-start] Writing kubelet configuration to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml&quot;14[kubelet-start] Starting the kubelet15[certs] Using certificateDir folder &quot;&#x2F;etc&#x2F;kubernetes&#x2F;pki&quot;16[certs] Generating &quot;ca&quot; certificate and key17[certs] Generating &quot;apiserver&quot; certificate and key18[certs] apiserver serving cert is signed for DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.31.0.57]19[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key20[certs] Generating &quot;front-proxy-ca&quot; certificate and key21[certs] Generating &quot;front-proxy-client&quot; certificate and key22[certs] Generating &quot;etcd&#x2F;ca&quot; certificate and key23[certs] Generating &quot;etcd&#x2F;server&quot; certificate and key24[certs] etcd&#x2F;server serving cert is signed for DNS names [master localhost] and IPs [172.31.0.57 127.0.0.1 ::1]25[certs] Generating &quot;etcd&#x2F;peer&quot; certificate and key26[certs] etcd&#x2F;peer serving cert is signed for DNS names [master localhost] and IPs [172.31.0.57 127.0.0.1 ::1]27[certs] Generating &quot;etcd&#x2F;healthcheck-client&quot; certificate and key28[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key29[certs] Generating &quot;sa&quot; key and public key30[kubeconfig] Using kubeconfig folder &quot;&#x2F;etc&#x2F;kubernetes&quot;31[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file32[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file33[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file34[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file35[control-plane] Using manifest folder &quot;&#x2F;etc&#x2F;kubernetes&#x2F;manifests&quot;36[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;37[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;38W0119 11:04:31.392140 10036 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;39[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;40W0119 11:04:31.393831 10036 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;41[etcd] Creating static Pod manifest for local etcd in &quot;&#x2F;etc&#x2F;kubernetes&#x2F;manifests&quot;42[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;&#x2F;etc&#x2F;kubernetes&#x2F;manifests&quot;. This can take up to 4m0s43[kubelet-check] Initial timeout of 40s passed.44[apiclient] All control plane components are healthy after 56.506369 seconds45[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace46[kubelet] Creating a ConfigMap &quot;kubelet-config-1.17&quot; in namespace kube-system with the configuration for the kubelets in the cluster47[upload-certs] Skipping phase. Please see --upload-certs48[mark-control-plane] Marking the node master as control-plane by adding the label &quot;node-role.kubernetes.io&#x2F;master&#x3D;&#39;&#39;&quot;49[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io&#x2F;master:NoSchedule]50[bootstrap-token] Using token: 1octc9.3uvkgo5vy1sgr7vy51[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles52[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials53[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token54[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster55[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace56[kubelet-finalize] Updating &quot;&#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf&quot; to point to a rotatable kubelet client certificate and key57[addons] Applied essential addon: CoreDNS58[addons] Applied essential addon: kube-proxy5960Your Kubernetes control-plane has initialized successfully!6162To start using your cluster, you need to run the following as a regular user:6364 mkdir -p $HOME&#x2F;.kube65 sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config66 sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config6768You should now deploy a pod network to the cluster.69Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:70 https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;addons&#x2F;7172Then you can join any number of worker nodes by running the following on each as root:7374kubeadm join 172.31.0.57:6443 --token 1octc9.3uvkgo5vy1sgr7vy \\75 --discovery-token-ca-cert-hash sha256:72fc3bf1a6cc0a159a27bf99518c116845b6a8ae05fb8e78cddc4c7156777c10 部署完成后，kubeadm 会生成一行指令： 1kubeadm join 172.31.0.57:6443 --token 1octc9.3uvkgo5vy1sgr7vy \\2 --discovery-token-ca-cert-hash sha256:72fc3bf1a6cc0a159a27bf99518c116845b6a8ae05fb8e78cddc4c7156777c10 这个命令，就是用来给这个Master节点添加更多的工作节点使用的。 另外，kubeadm 还会提示我们一些配置命令： 1mkdir -p $HOME&#x2F;.kube2sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config3sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config 这样配置的原因是：Kubernetes 集群默认需要加密方式访问。所以将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。 现在，用 kubectl get 命令查看当前唯一一个节点的状态： 1root@master:~# kubectl get nodes2NAME STATUS ROLES AGE VERSION3master NotReady master 2m26s v1.17.145root@master:~# kubectl get cs 6NAME STATUS MESSAGE ERROR7scheduler Healthy ok 8controller-manager Healthy ok 9etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; 输出结果中，master 的状态是 NotReady，排查问题，最重要的手段就是用 kubectl describe 来查看这个节点（Node）对象的详细信息、状态和事件（Event）。 1root@master:~# kubectl describe node master23.....4Conditions:5 Type Status LastHeartbeatTime LastTransitionTime Reason Message6 ---- ------ ----------------- ------------------ ------ -------7 MemoryPressure False Sun, 19 Jan 2020 11:07:23 +0800 Sun, 19 Jan 2020 11:07:23 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available8 DiskPressure False Sun, 19 Jan 2020 11:07:23 +0800 Sun, 19 Jan 2020 11:07:23 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure9 PIDPressure False Sun, 19 Jan 2020 11:07:23 +0800 Sun, 19 Jan 2020 11:07:23 +0800 KubeletHasSufficientPID kubelet has sufficient PID available10 Ready False Sun, 19 Jan 2020 11:07:23 +0800 Sun, 19 Jan 2020 11:07:23 +0800 KubeletNotReady runtime network not ready: NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized1112..... 从输出中，可以看到原因在于为部署任何网络插件，network plugin is not ready: cni config uninitialized。 我们还可以通过 kubectl 查看这个节点上各个系统 Pod 的状态，其中 kube-system 是 Kubernetes 项目预留给系统 Pod 的工作空间。 1root@master:~# kubectl get pods -n kube-system 2NAME READY STATUS RESTARTS AGE3coredns-9d85f5447-fksl4 0&#x2F;1 Pending 0 6m45s4coredns-9d85f5447-qqkwn 0&#x2F;1 Pending 0 6m44s5etcd-master 1&#x2F;1 Running 0 7m2s6kube-apiserver-master 1&#x2F;1 Running 0 7m2s7kube-controller-manager-master 1&#x2F;1 Running 0 7m1s8kube-proxy-h2kdx 1&#x2F;1 Running 0 6m45s9kube-scheduler-master 1&#x2F;1 Running 0 7m1s 从输出中可以看到，coredns 的 Pod 处于 Pending 状态，即调度失败，当然也符合预期，因为这个 Master 节点的网络尚未就绪。 3.部署网络插件本次我们选择 Weave 为网络插件，只需执行一句kubectl apply指令 1root@master:~# kubectl apply -f &quot;https:&#x2F;&#x2F;cloud.weave.works&#x2F;k8s&#x2F;net?k8s-version&#x3D;$(kubectl version | base64 | tr -d &#39;\\n&#39;)&quot;2serviceaccount&#x2F;weave-net created3clusterrole.rbac.authorization.k8s.io&#x2F;weave-net created4clusterrolebinding.rbac.authorization.k8s.io&#x2F;weave-net created5role.rbac.authorization.k8s.io&#x2F;weave-net created6rolebinding.rbac.authorization.k8s.io&#x2F;weave-net created7daemonset.apps&#x2F;weave-net created 部署完，等一两分钟，再次检查pod状态 1root@master:~# kubectl get pods -n kube-system2NAME READY STATUS RESTARTS AGE3coredns-9d85f5447-fksl4 1&#x2F;1 Running 0 12m4coredns-9d85f5447-qqkwn 1&#x2F;1 Running 0 12m5etcd-master 1&#x2F;1 Running 0 12m6kube-apiserver-master 1&#x2F;1 Running 0 12m7kube-controller-manager-master 1&#x2F;1 Running 0 12m8kube-proxy-h2kdx 1&#x2F;1 Running 0 12m9kube-scheduler-master 1&#x2F;1 Running 0 12m10weave-net-xssqj 2&#x2F;2 Running 0 95s 现在，所有的系统 Pod 都成功启动了，Weave 插件新建了一个名叫 weave-net-xssqj 的 Pod，这就是容器网络插件在每个节点上的控制组件。 4.部署 Kubernetes 的 Worker 节点Kubernetes 的 Worker 节点和 Master 节点几乎是相同的，它们运行的都是一个 kubelet 组件。区别在于，Master 节点上多运行这 kube-apiserver、kube-scheduler、kube-controller-manager 这三个系统 Pod。 部署 Worker 节点是最简单的，只需要两步即可。 第一步，在所有的 Worker 节点上执行“安装 Docker 和 kubeadm”这一步的操作 第二步，执行部署 Master 节点时生成的kubeadm join命令 1kubeadm join 172.31.0.57:6443 --token 1octc9.3uvkgo5vy1sgr7vy \\2 --discovery-token-ca-cert-hash sha256:72fc3bf1a6cc0a159a27bf99518c116845b6a8ae05fb8e78cddc4c7156777c10 这时，我们在 Master 节点上查看一下当前集群的节点（node） 1root@master:~# kubectl get node2NAME STATUS ROLES AGE VERSION3master Ready master 28h v1.17.14worker01 Ready &lt;none&gt; 27h v1.17.15worker02 Ready &lt;none&gt; 21h v1.17.1 可以看到，当前集群有三个节点，并且都是 Ready。 5.通过 Taint 调整 Master 执行 Pod 的策略 备注：默认情况下，Master 节点是不允许运行用户的 Pod 的。 原理比较简单：一旦某个节点被打上了一个 Taint，即“有了污点”，那么所有的 Pod 就都不能在这个节点上运行。 通过 kubectl descirbe 检查一下 Master 节点的 Taint 字段， 1root@master:~# kubectl describe node master2Name: master3Roles: master4Labels: ......5Annotations: ......6CreationTimestamp: ......7Taints: node-role.kubernetes.io&#x2F;master:NoSchedule8...... 可以看到，Master节点默认加上了一个node-role.kubernetes.io/master:NoSchedule这样的Taint，键是node-role.kubernetes.io/master，值是NoSchedule。 所以，需要将这个Taint删掉才行 1root@master:~# kubectl taint nodes --all node-role.kubernetes.io&#x2F;master- 我们在node-role.kubernetes.io/master这个键后面加上了一个短横线-，这个格式就意味着移除所有以node-role.kubernetes.io/master为键的 Taint。 至此，一个基本完整的 Kubernetes 集群就部署完毕了。 接下来，会再安装一些其他的辅助插件，如 Dashboard 和存储插件。 6.部署 Dashboard 可视化插件（可选）Dashboard 的部署也很简单，还是通过kubectl apply命令 1root@master:~# wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes&#x2F;dashboard&#x2F;v2.0.0-rc2&#x2F;aio&#x2F;deploy&#x2F;recommended.yaml2root@master:~# mv recommended.yaml kubernetes-dashboard.yaml3root@master:~# kubectl apply -f kubernetes-dashboard.yaml4namespace&#x2F;kubernetes-dashboard created5serviceaccount&#x2F;kubernetes-dashboard created6service&#x2F;kubernetes-dashboard created7secret&#x2F;kubernetes-dashboard-certs created8secret&#x2F;kubernetes-dashboard-csrf created9secret&#x2F;kubernetes-dashboard-key-holder created10configmap&#x2F;kubernetes-dashboard-settings created11role.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard created12clusterrole.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard created13rolebinding.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard created14clusterrolebinding.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard created15deployment.apps&#x2F;kubernetes-dashboard created16service&#x2F;dashboard-metrics-scraper created17deployment.apps&#x2F;dashboard-metrics-scraper created 部署完成后，在kubernetes-dashboard的命名空间下，会新建两个 Pod。 1root@master:~# kubectl get pod -n kubernetes-dashboard2NAME READY STATUS RESTARTS AGE3dashboard-metrics-scraper-7b64584c5c-w46jk 1&#x2F;1 Running 0 3m4kubernetes-dashboard-566f567dc7-554zh 1&#x2F;1 Running 0 3m 注意：Dashboard 部署完成后，默认只能通过 Proxy 的方式在本地访问。如果想从集群外访问这个 Dashboard 的话，需要用到 Ingress。 7.部署容器存储插件本次部署选择 Rook 存储插件。 1root@master:~# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;rook&#x2F;rook&#x2F;master&#x2F;cluster&#x2F;examples&#x2F;kubernetes&#x2F;ceph&#x2F;common.yaml2root@master:~# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;rook&#x2F;rook&#x2F;master&#x2F;cluster&#x2F;examples&#x2F;kubernetes&#x2F;ceph&#x2F;operator.yaml3root@master:~# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;rook&#x2F;rook&#x2F;master&#x2F;cluster&#x2F;examples&#x2F;kubernetes&#x2F;ceph&#x2F;cluster.yaml 部署完成后，可以看到 Rook 在rook-ceph的命名空间下新建了一些 Pod。 1root@master:~# kubectl get pod -n rook-ceph 2NAME READY STATUS RESTARTS AGE3csi-cephfsplugin-4t7qj 3&#x2F;3 Running 0 4m4csi-cephfsplugin-9db7n 3&#x2F;3 Running 0 4m5csi-cephfsplugin-pb8pv 3&#x2F;3 Running 0 4m6csi-cephfsplugin-provisioner-8b9d48896-2qk22 4&#x2F;4 Running 0 4m7csi-cephfsplugin-provisioner-8b9d48896-dgbkm 4&#x2F;4 Running 1 4m8csi-rbdplugin-4ln69 3&#x2F;3 Running 0 4m9csi-rbdplugin-7qp7d 3&#x2F;3 Running 0 4m10csi-rbdplugin-provisioner-6d465d6c6f-4jsfj 5&#x2F;5 Running 0 4m11csi-rbdplugin-provisioner-6d465d6c6f-w4gzx 5&#x2F;5 Running 1 4m12csi-rbdplugin-sqmvz 3&#x2F;3 Running 0 4m13rook-ceph-crashcollector-master-6b8dd5d4fd-thq4w 1&#x2F;1 Running 0 3m14rook-ceph-crashcollector-worker01-68d69757d7-j9fzs 1&#x2F;1 Running 0 4m15rook-ceph-crashcollector-worker02-8474bd88d-xftgn 1&#x2F;1 Running 0 3m16rook-ceph-mgr-a-76c6c49cc9-7tlzb 1&#x2F;1 Running 0 3m17rook-ceph-mon-a-59998d787-bdvlv 1&#x2F;1 Running 0 4m18rook-ceph-mon-b-6d69bddc95-vvvkq 1&#x2F;1 Running 0 4m19rook-ceph-mon-c-776d969f6c-89fhm 1&#x2F;1 Running 0 4m20rook-ceph-operator-678887c8d-sm8bq 1&#x2F;1 Running 0 3m21rook-ceph-osd-0-78fd8856df-lrfjx 1&#x2F;1 Running 0 3m22rook-ceph-osd-1-5cd4b9564f-k55xt 1&#x2F;1 Running 0 3m23rook-ceph-osd-2-7c4d99694f-j8phk 1&#x2F;1 Running 0 3m24rook-ceph-osd-prepare-master-8f69h 0&#x2F;1 Completed 0 3m25rook-ceph-osd-prepare-worker01-pm6cq 0&#x2F;1 Completed 0 3m26rook-ceph-osd-prepare-worker02-dg7gb 0&#x2F;1 Completed 0 3m27rook-discover-bpnc7 1&#x2F;1 Running 0 4m28rook-discover-bs9hj 1&#x2F;1 Running 0 4m29rook-discover-wwvr2 1&#x2F;1 Running 0 4m rook-ceph-osd-prepare-master-8f69h、rook-ceph-osd-prepare-worker01-pm6cq 、rook-ceph-osd-prepare-worker02-dg7gb 它们的状态是 Completed，这个没有影响，其实它们是一种 job，执行完之后就变成 Completed 了。 另外，默认启动的Ceph集群，是开启Ceph认证的，这样你登陆Ceph组件所在的Pod里，是没法去获取集群状态，以及执行CLI命令，这时需要部署Ceph toolbox，tools spec如toolbox.yaml所示（参考官网）： 1apiVersion: apps&#x2F;v12kind: Deployment3metadata:4 name: rook-ceph-tools5 namespace: rook-ceph6 labels:7 app: rook-ceph-tools8spec:9 replicas: 110 selector:11 matchLabels:12 app: rook-ceph-tools13 template:14 metadata:15 labels:16 app: rook-ceph-tools17 spec:18 dnsPolicy: ClusterFirstWithHostNet19 containers:20 - name: rook-ceph-tools21 image: rook&#x2F;ceph:v1.2.222 command: [&quot;&#x2F;tini&quot;]23 args: [&quot;-g&quot;, &quot;--&quot;, &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;toolbox.sh&quot;]24 imagePullPolicy: IfNotPresent25 env:26 - name: ROOK_ADMIN_SECRET27 valueFrom:28 secretKeyRef:29 name: rook-ceph-mon30 key: admin-secret31 securityContext:32 privileged: true33 volumeMounts:34 - mountPath: &#x2F;dev35 name: dev36 - mountPath: &#x2F;sys&#x2F;bus37 name: sysbus38 - mountPath: &#x2F;lib&#x2F;modules39 name: libmodules40 - name: mon-endpoint-volume41 mountPath: &#x2F;etc&#x2F;rook42 # if hostNetwork: false, the &quot;rbd map&quot; command hangs, see https:&#x2F;&#x2F;github.com&#x2F;rook&#x2F;rook&#x2F;issues&#x2F;202143 hostNetwork: true44 volumes:45 - name: dev46 hostPath:47 path: &#x2F;dev48 - name: sysbus49 hostPath:50 path: &#x2F;sys&#x2F;bus51 - name: libmodules52 hostPath:53 path: &#x2F;lib&#x2F;modules54 - name: mon-endpoint-volume55 configMap:56 name: rook-ceph-mon-endpoints57 items:58 - key: data59 path: mon-endpoints 执行 rook-ceph-tools的 pod 1root@master:~# kubectl apply -f toolbox.yaml 部署完成后，可以看到在rook-ceph的命名空间下多了rook-ceph-tools-856c5bc6b4-mw7t8 Pod 1root@master:~# kubectl get pod -n rook-ceph| grep tools2rook-ceph-tools-856c5bc6b4-mw7t8 1&#x2F;1 Running 0 3m 现在，进入到pod当中，执行ceph -s，查看ceph 集群的状态 1root@master:~# kubectl exec -it rook-ceph-tools-856c5bc6b4-mw7t8 -n rook-ceph bash23[root@worker02 &#x2F;]# ceph -s4 cluster:5 id: 11436e29-c940-4a4f-9875-c65fac3531c16 health: HEALTH_OK7 8 services:9 mon: 3 daemons, quorum a,b,c (age 21h)10 mgr: a(active, since 45m)11 osd: 3 osds: 3 up (since 5h), 3 in (since 5h)12 13 data:14 pools: 0 pools, 0 pgs15 objects: 0 objects, 0 B16 usage: 3.0 GiB used, 294 GiB &#x2F; 297 GiB avail17 pgs: 可以看到，ceph集群是HEALTH_OK的。","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2019-12-09T09:47:09.200Z","updated":"2019-12-09T09:47:09.201Z","comments":true,"path":"2019/12/09/hello-world/","link":"","permalink":"https://zhaoyixin.xyz/2019/12/09/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}